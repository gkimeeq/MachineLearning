**1.问题提出**

已知数据样本：

```math
X = \left( \begin{matrix}
x_1^{(1)} &x_2^{(1)} &\cdots &x_n^{(1)}\\
x_1^{(2)} &x_2^{(2)} &\cdots &x_n^{(2)}\\
\vdots &\vdots &\ddots &\vdots\\
x_1^{(m)} &x_2^{(m)} &\cdots &x_n^{(m)}
\end{matrix} \right)
,
Y = \left( \begin{matrix}
y^{(1)}\\
y^{(2)}\\
\vdots\\
y^{(m)}
\end{matrix} \right)
```

其中$m$是样本数，$n$是$X$的特征维度。目标是从这些已经的数据中，找到一种线性关系，利用这种线性关系，对于新的$(x_1^{(m+1)}, x_2^{(m+1)}, \cdots , x_n^{(m+1)})$，得出$y^{(m+1)}$。

**2.建立模型**

对于$n$维的数据$x$，对应的线性模型为：

$$
h_{(\theta_0, \theta_1, \theta_2, \cdots, \theta_n)}(x_1, x_2,\cdots, x_n) = \theta_0 + \theta_1 * x_1 + \theta_2 * x_2 + \cdots + \theta_n * x_n = \sum_{i=0}^n\theta_ix_i
$$

其中$\theta_i, (i = 0, 1, 2, \cdots, n)$为参数，而$x_i, (i = 0, 1, 2, \cdots, n)$为样本$n$个维度对应的值，这里$i=0$时，定义$x_0=1$。

定义$\theta$为列向量，即$\theta = (\theta_0, \theta_1, \theta_2, \cdots, \theta_n)^T$，线性模型可以写成矩阵形式（$X$是包含有$x_0$的矩阵）：

$$
h_\theta (X) = X\theta
$$

$X$为$m\times (n+1)$的矩阵，$h_\theta(X)$是$m\times 1$的列向量。

**3.推导**

由中心极限定理，在实际问题中，很多随机现象都可以看做是众多因素的独立影响的综合反应（多个随机变量的和），这种随机现象近似服从正态分布。

对于第$i$个样本$x^{(i)}$，由参数$\theta^T$得到的预测值为$\theta^Tx^{(i)}$，与实际值$y^{(i)}$相差为$\epsilon^{(i)}$，即$y^{(i)} = \theta^Tx^{(i)}+\epsilon ^{(i)}$，这个$\epsilon^{(i)},(1 \leq i \leq m)$就是每个样本的误差，且由中心极限定理，它是独立同分布的，服从均值为0，方差为某个定值$\sigma^2$的正态分布。

$\epsilon^{(i)}$服从正态分布，由似然函数得：

$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(\epsilon^{(i)})^2}{2\sigma^2} \right)
$$

$\epsilon^{(i)}$的概率密度，也就是在$\theta$这个变量下，给定了样本$x^{(i)}$，$y^{(i)}$的概率密度。把$y^{(i)} = \theta^Tx^{(i)}+\epsilon ^{(i)}$代入可得：

$$
p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2} \right)
$$

对于所有$1 \leq i \leq m$的样本，联合概率密度为（似然函数，$\theta$是变量，要求解的对象）：

$$
\begin{align}
L(\theta) &= \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\
&=\prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2} \right)
\end{align}
$$

对数似然则为：

$$
\begin{align}
l(\theta) &= \log L(\theta) \\
&= \log \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\
&=\sum_{i=1}^m \log \left( \frac{1}{\sqrt{2\pi}\sigma} \exp\left( -\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2} \right) \right)\\
&=\sum_{i=1}^m \left( \log \left( \frac{1}{\sqrt{2\pi}\sigma} \right) + \left( -\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2} \right) \right)\\
&=m \log \left( \frac{1}{\sqrt{2\pi}\sigma} \right) - \frac{1}{\sigma^2}\cdot \frac{1}{2}\sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)})^2
\end{align}
$$

求解$\theta$的目标是使得似然函数取得最大值，亦即对数似然取得最大值。由上式，$\sigma$为某一个定值，在给定了样本的情况下，$m \log \left( \frac{1}{\sqrt{2\pi}\sigma} \right)$也为定值。因此当$\frac{1}{2}\sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)})^2$取得最小值时，对数似然取得最大值，最终的目标函数为：

$$
J(\theta) = \frac{1}{2}\sum_{i=1}^m (y^{(i)} - \theta^Tx^{(i)})^2
$$

即求得到$\theta$要使目标函数取得最小值。而上面的目标函数，与最小二乘法的目标函数是一致的。写成矩阵的形式为：

$$
J(\theta) = \frac{1}{2}(Y-X\theta)^T(Y-X\theta)
$$

**4.解析解求解**

由上面的目标函数，可以让梯度为0，从而求出驻点。

$$
\begin{align}
\frac{\partial J(\theta)}{\partial \theta} &= \frac{\partial}{\partial\theta} \left( \frac{1}{2}(Y-X\theta)^T(Y-X\theta) \right)\\
&= \frac{\partial}{\partial\theta} \left( \frac{1}{2}(Y^T-\theta^TX^T)(Y-X\theta) \right)\\
&= \frac{\partial}{\partial\theta} \left( \frac{1}{2}(Y^TY-Y^TX\theta-\theta^TX^TY+\theta^TX^TX\theta) \right)\\
&= \frac{1}{2}(-X^TY-X^TY+2X^TX\theta)\\
&= X^TX\theta - X^TY
\end{align}
$$

令$\frac{\partial J(\theta)}{\partial \theta}=X^TX\theta - X^TY=0$，则有$X^TX\theta= X^TY => \theta = (X^TX)^{-1}X^TY$。

由解析解可知，式中要求$X^TX$可逆。若不可逆，或者为了防止过拟合，往往会增加$\lambda,(\lambda > 0)$扰动，则

$$
\theta = (X^TX+\lambda I)^{-1}X^TY
$$

其中$I$为单位矩阵。

对于任意矩阵$X$，任意的非零向量$u$，令$v = Xu$，有$u^TX^TXu = (Xu)^TXu=v^Tv \geq 0$，所以矩阵$X^TX$是半正定的，对于任意的实数$\lambda > 0$，$X^TX+\lambda I$正定，从而可逆，保证解析解有意义。
