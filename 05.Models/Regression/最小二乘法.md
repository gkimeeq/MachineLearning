**1.目标函数**

已知数据样本：

$$
X = \left( \begin{matrix}
x_1^{(1)} &x_2^{(1)} &\cdots &x_n^{(1)}\\
x_1^{(2)} &x_2^{(2)} &\cdots &x_n^{(2)}\\
\vdots &\vdots &\ddots &\vdots\\
x_1^{(m)} &x_2^{(m)} &\cdots &x_n^{(m)}
\end{matrix} \right)
,
Y = \left( \begin{matrix}
y^{(1)}\\
y^{(2)}\\
\vdots\\
y^{(m)}
\end{matrix} \right)
$$

其中$m$是样本数，$n$是$X$的特征维度。$y^{(i)}, i=1, 2, \cdots, m$为对应的样本得到的观测值。

假设通过某种手段（算法）得到$x^{(i)}$与$y^{(i)}$的模型关系为$\widehat {y^{(i)}} = f(x^{(i)})$。这里$\widehat {y^{(i)}}$对应着预测值。最小二乘法的目标函数（损失函数）定义为：

$$
J(\theta) = \frac{1}{2}\sum_{i=0}^m\left( y^{(i)} - \widehat {y^{(i)}} \right)^2 = \frac{1}{2}\sum_{i=0}^m\left( y^{(i)} - f(x^{(i)};\theta) \right)^2
$$

这里$\frac{1}{2}$是为了后面求解消去2，$\theta$为$x$到$y$映射关系函数$f$的参数，是求解的对象。要通过目标函数来求解$\theta$，使得$\theta$能令目标函数取得最小值。

**2.求解**

要求$J(\theta)$取得最小值时$\theta$的值，可以通过对$J(\theta)$的$\theta$进行求导，然后令导数为零得到方程（组）进行求解。假设$f(x^{(i)};\theta)$对$\theta$的一次偏导存在，为$\frac {\partial f(x^{(i)};\theta)} {\partial \theta}$，则：

$$
\frac {\partial J(\theta)} {\partial \theta} = -\sum_{i=0}^m\left( y^{(i)} - f(x^{(i)};\theta) \right) \frac {\partial f(x^{(i)};\theta)} {\partial \theta}
$$

令上式等于0，$\sum_{i=0}^m\left( y^{(i)} - f(x^{(i)};\theta) \right) \frac {\partial f(x^{(i)};\theta)} {\partial \theta}=0$即可求出$\theta$。

**3.线性关系的代数解法**

如果$f$为线性的关系，即（增加$x_0^{(i)}=1$的项）

$$
f(x^{(i)};\theta)=\theta_0x_0^{(i)} + \theta_1x_1^{(i)} + \cdots + \theta_nx_n^{(i)} = \sum_{j=0}^n\theta_jx_j^{(i)},  i=1, 2, \cdots, m
$$

于是有$\frac {\partial f(x^{(i)};\theta)} {\partial \theta_j}=x_j^{(i)}$，代入$\frac {\partial J(\theta)} {\partial \theta}$得：

$$
\frac {\partial J(\theta)} {\partial \theta_j} = -\sum_{i=0}^m\left( y^{(i)} -\sum_{j=0}^n\theta_jx_j^{(i)} \right) x_j^{(i)}
$$

对于$j=0, 1, 2, \cdots, n$，由上式等于0可以组成$n+1$个方程的方程组：

$$
\begin{cases}
\sum_{i=0}^m\left( y^{(i)} -\sum_{j=0}^n\theta_jx_j^{(i)} \right) x_0^{(i)} = 0\\
\sum_{i=0}^m\left( y^{(i)} -\sum_{j=0}^n\theta_jx_j^{(i)} \right) x_1^{(i)}=0\\
\sum_{i=0}^m\left( y^{(i)} -\sum_{j=0}^n\theta_jx_j^{(i)} \right) x_2^{(i)}=0\\
\cdots \\
\sum_{i=0}^m\left( y^{(i)} -\sum_{j=0}^n\theta_jx_j^{(i)} \right) x_n^{(i)}=0
\end{cases}
$$

解这个方程组即可得到使得目标函数取最小值的$\theta$。

**4.线性关系的矩阵解法**

由$f(x^{(i)};\theta)=\theta_0x_0^{(i)} + \theta_1x_1^{(i)} + \cdots + \theta_nx_n^{(i)}$，把所有样本写成矩阵的形式为$f(X;\theta)=X\theta$，这里的$X$增加了$x_0=1$的项，为$m\times (n+1)$的矩阵，$\theta$为$n+1$的列向量。目标函数可以写为：

$$
\begin{aligned}
J(\theta) &= \frac{1}{2}(X\theta-Y)^T(X\theta-Y)\\
&=\frac{1}{2}(\theta^TX^T-Y^T)(X\theta-Y)\\
&=\frac{1}{2}(\theta^TX^TX\theta - \theta^TX^TY-Y^TX\theta + Y^TY)
\end{aligned}
$$

对$\theta$求导得：

$$
\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{2}(2X^TX\theta - X^TY - (Y^TX)^T) = \frac{1}{2}(2X^TX\theta - 2X^TY) = X^TX\theta - X^TY
$$

令其为0，得$X^TX\theta = X^TY$，然后容易求得：

$$
\theta = (X^TX)^{-1}X^TY
$$

**5.注意**

求解的过程，要求$X^TX$的逆矩阵，如果数据量很大，是很耗时的。

逆矩阵是有可能不存在，这时可以加入大于0的扰动$\lambda$，即求$X^TX+\lambda I$的逆矩阵。

对于$f$是线性的，上面的推导说明是可以求解的，如果$f$不是线性的，可能会导致无法求解。
